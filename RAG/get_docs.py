import requests
from bs4 import BeautifulSoup
import os
import re
import time
from url import URLS

# --- CONFIGURACI√ìN ---
OUTPUT_DIR = "RAG/docs"  # Carpeta donde se guardar√°n los .txt


def clean_filename(title):
    """Convierte un t√≠tulo en un nombre de archivo v√°lido."""
    return re.sub(r'[\\/*?:"<>|]', "", title).strip().replace(" ", "_")

def clean_text(text):
    """
    Limpia el texto para que sea digerible por el LLM.
    Elimina espacios extra, saltos de l√≠nea m√∫ltiples, etc.
    """
    # 1. Separamos el texto por l√≠neas originales
    lines = text.split(". ")
    
    # 2. Limpiamos cada l√≠nea individualmente (quitamos espacios al inicio/final)
    # y filtramos las l√≠neas vac√≠as.
    cleaned_lines = []
    for line in lines:
        
        # Reemplazar m√∫ltiples espacios/tabs por un solo espacio
        text = re.sub(r'\s+', ' ', line)
        # Reemplazar espacios antes de puntos/comas
        text = re.sub(r'\s([?.!"])', r'\1',text)
        # Solo guardamos la l√≠nea si tiene contenido real
        if text:
            cleaned_lines.append(text)
            
    # 3. Unimos las l√≠neas con un salto de l√≠nea para que sea un texto vertical
    # Usamos '\n' para separar frases o '\n\n' si prefieres p√°rrafos muy marcados.
    return "\n".join(cleaned_lines)
    return text.strip()

def scrape_url(url):
    print(f"üîÑ Descargando: {url}")
    
    # Headers para parecer un navegador real y evitar bloqueos (403 Forbidden)
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }

    try:
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status() # Lanza error si la web falla
        
        # Parsear HTML
        soup = BeautifulSoup(response.text, 'html.parser')

        # 1. ELIMINAR RUIDO (Scripts, Estilos, Men√∫s, Pies de p√°gina)
        # Esto es CR√çTICO para que el LLM no lea c√≥digo basura.
        for tag in soup(["script", "style", "nav", "footer", "header", "aside", "form", "iframe"]):
            tag.decompose() # Elimina la etiqueta del √°rbol

        # 2. Extraer T√≠tulo para el nombre del archivo
        page_title = soup.title.string if soup.title else "documento_sin_titulo"
        filename = clean_filename(page_title) + ".txt"

        # 3. Extraer Texto Principal (Priorizamos p√°rrafos y encabezados)
        # Buscamos el contenedor principal si es posible (com√∫n en blogs/art√≠culos)
        content_div = soup.find('main') or soup.find('article') or soup.body
        
        # Obtener texto separando bloques por saltos de l√≠nea
        raw_text = content_div.get_text(separator='\n\n')

        # 4. Limpieza final
        final_text = clean_text(raw_text)

        # A√±adimos la URL al principio del texto para referencia del RAG
        final_content = f"FUENTE: {url}\nTITULO: {page_title}\n\n{final_text}"

        # 5. Guardar archivo
        file_path = os.path.join(OUTPUT_DIR, filename)
        with open(file_path, "w", encoding="utf-8") as f:
            f.write(final_content)
        
        print(f"‚úÖ Guardado en: {file_path}")
        return True

    except Exception as e:
        print(f"‚ùå Error descargando {url}: {e}")
        return False

def main():
    # Crear carpeta si no existe
    if not os.path.exists(OUTPUT_DIR):
        os.makedirs(OUTPUT_DIR)
        print(f"üìÇ Carpeta '{OUTPUT_DIR}' creada.")

    success_count = 0
    for url in URLS:
        if scrape_url(url):
            success_count += 1
        # Peque√±a pausa para no saturar servidores (cortes√≠a web)
        time.sleep(1) 

    print(f"\n‚ú® Proceso terminado. {success_count}/{len(URLS)} documentos procesados.")

if __name__ == "__main__":
    main()